# CS229 课程讲义中文翻译
CS229 Section notes

|原作者|翻译|
|---|---|
|Daniel Ramage|[XiaoDong_Wang](https://github.com/Dongzhixiao) |


|相关链接|
|---|
|[Github 地址](https://github。com/Kivy-CN/Stanford-CS-229-CN)|
|[知乎专栏](https://zhuanlan。zhihu。com/MachineLearn)|
|[斯坦福大学 CS229 课程网站](http://cs229。stanford。edu/)|
|[网易公开课中文字幕视频](http://open。163。com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC。html)|


### 隐马尔可夫模型的基本原理

#### 摘要

我们如何将机器学习应用于随时间变化观察到的一系列数据中来？例如，我们可能对根据一个人讲话的录音来发现他所说的话的顺序感兴趣。或者，我们可能对用词性标记来注释单词序列感兴趣。本小节的内容对马尔可夫模型的概念进行了全面的数学介绍，该模型是一种关于状态随时间变化的推理一种学习形式。并且使用隐马尔可夫模型，我们希望从一系列观察数据中恢复这一系列模型的初始状态。最后一节包含一些特定参考资料，这些资料从其他角介绍隐马尔可夫模型。

#### 1. 马尔科夫模型

给定一个状态集合$S=\{s_1,s_2,\dots,s_{|s|}\}$，我们可以观察到一系列随时间变化的序列$\vec{z}\in S^T$。例如，我们也许有这样一个来自天气系统的状态集合$S=\{sun,cloud,rain\}$，显然$|S|=3$。在给定$T=5$的情况下我们可能会观察到这几天的天气情况的一个序列$\{z_1=s_{sun},z_2=s_{cloud},z_3=s_{cloud},z_4=s_{rain},z_5=s_{cloud}\}$

我们上面的天气示例里面的观察状态可以表示随时间变化的一种随机过程的输出。如果没有进一步的假设，时间$t$下的状态$s_j$可以是变量为任意数的一个函数，包括从时间$1$到$t-1$的所有状态，可能还有许多其他我们甚至没有建模的状态。然而，我们将做两个马尔可夫假设，这将允许我们对时间序列进行可以追溯的推断。

有限地平线假设(limited horizon assumption)是$t$时刻处于状态的概率只取决于$t-1$时刻的状态。这个假设背后的直觉是，$t$时刻的状态代表对过去“足够”的总结，可以合理地预测未来。正式的公式如下:

$$
P(z_t|z_{t-1},z_{t-2},\dots,z_1)=P(z_t|z_{t-1})
$$

平稳过程假设(stationary process assumption)是在给定当前状态的条件下，下一个状态的条件分布不随时间变化。正式的公式如下:

$$
P(z_t|z_{t-1})=P(z_2|z_1);t\in 2\dots T
$$

习惯上，我们还将假设存在一个初始状态和初始观察值$z_0\equiv s_0$，其中$s_0$为$0$时刻状态的初始概率分布。这种符号定义可以使我们方便编码观察到第一个真实的状态$z_1$的先验概率的确信度，其可以用符号表示为$p(z_1|z_0)$。注意到公式$P(z_t|z_{t-1},\dots,z_1)=P(z_t|z_{t-1},\dots,z_1,z_0)$的出现是因为我们为所有状态都定义了$z_0=s_0$。（HMMs的其他表示形式有时用向量$\pi\in R^{|S|}$表示这些先验确信度(prior believes)）

我们通过定义一个状态转移矩阵$A\in R^{(|S|+1)\times(|S|+1)}$来参数化这些转移数据。矩阵中的值$A_{ij}$代表在任意时刻 从状态$i$转移到状态$j$的转移概率。对于我们太阳和雨的例子，可能有以下转换矩阵：

$$
A=\begin{matrix}
\ & s_0&s_{sun} & s_{cloud} & s_{rain}\\
s_0 &  0 & .33 & .33 & .33 \\
s_{sun} & 0 & .8 & .1 & .1 \\
s_{cloud} & 0 & .2 & .6 & .2\\
s_{rain} & 0 & .1 & .2 & .7
\end{matrix}
$$

请注意，这些数字（我自己编的）表明了天气是自相关的，这是因为：如果天气晴朗，它将趋向于保持晴朗，如果天气多云将保持多云，等等。这种模式在许多马尔可夫模型中都很常见，可以作为转移矩阵中的强对角性来遵守。注意，在本例中，我们的初始状态$s_0$显示了过渡到天气系统中的三种状态的概率是一样的。

##### 1.1 马尔可夫模型的两个问题

结合马尔可夫假设和状态转移参数矩阵$A$，我们可以回答关于马尔可夫链中状态序列的两个基本问题。
- 给定一个特定的状态序列$\vec{z}$，其概率是多少？
- 给定一个观测序列$\vec{z}$，如何通过其进行最大似然估计得到状态转移参数矩阵$A$？

###### 1.1.1 状态序列的概率

我们可以利用概率的链式法则来计算某一特定状态序列$\vec{z}$的概率：

$$
\begin{aligned}
P（\vec{z}) &= P(z_t,z_{t-1},\dots,z_1;A) \\
&= P(z_t,z_{t-1},\dots,z_1,z_0;A) \\
&= P(z_t|z_{t-1},z_{t-2},\dots,z_1;A)P(z_{t-1}|z_{t-2},\dots,z_1;A)\dots P(z_1|z_0;A) \\
&= P(z_t|z_{t-1};A)P(z_{t-1}|z_{t-2};A)\dots P(z_2|z_1;A)P(z_1|z_0;A) \\
&= \prod_{t=1}^TP(z_t|z_{t-1};A) \\
&= \prod_{t=1}^TA_{z_{t-1} z_t}
\end{aligned}
$$

在第二行，我们引入$z_0$进入我们公式中的联合概率密度，这使得该式子可以通过上面定义的$z_0$来计算。第三行的结果是通过概率链式法则或贝叶斯规则的重复应用在该联合概率密度上得到的。第四行遵循马尔可夫假设，最后一行表示这些项都来自于转换矩阵$A$中的元素。

我们计算一下前面例子中的时间序列的概率。通过式子表达的话，即我们想要计算$P(z_1 = s_{sun} , z_2 = s_{cloud} , z_3 = s_{rain} , z_4 = s_{rain} , z_5 = s_{cloud})$，这个式子可以通过分解来计算，即$P(s_{sun}|s_0)P(s_{cloud}|s_{sun})P(s_{rain}|s_{cloud})P(s_{rain}|s_{rain})P(s_{cloud}|s_{rain}) =.33 \times .1 \times .2 \times .7 \times .2$。

###### 1.1.2 最大似然参数赋值

从学习的角度来看，我们可以通过观察序列$\vec{z}$的对数似然函数找到参数矩阵$A$。相应的找到从晴天到多云，以及相对的从晴天到晴天等转移的似然，这使得观察集合发生的概率最大。让我们定义一个马尔科夫模型的最大似然函数：

$$
\begin{aligned}
l(A) &= logP(\vec{z};A) \\
&= log\prod_{t=1}^TA_{z_{t-1} z_t} \\
&= \sum_{t=1}^TlogA_{z_{t-1} z_t} \\
&= \sum_{i=1}^{|S|}\sum_{j=1}^{|S|}\sum_{t=1}^{T}1\{z_{t-1}=s_i\wedge z_t=s_j\}logA_{ij}
\end{aligned}
$$

在最后一行中，我们使用一个示性函数，当条件保持不变时，它的值为$1$，否则为$0$，以在每个时间步长选择观察到的转换。在求解这一优化问题时，重要的是要保证所求解的参数矩阵$A$仍然是一个有效的转移矩阵。特别地，我们需要确保状态$i$的输出概率分布总是和为$1$，并且$A$的所有元素都是非负的。我们可以用拉格朗日乘子法来求解这个优化问题。

$$
\begin{aligned}
\max_A\qquad &l(A) \\
s.t.\qquad &\sum_{j=1}^{|S|}A_{ij}=1,\quad i=1..|S|\\
&A_{ij}\ge 0,\quad i,j=1..|S|
\end{aligned}
$$

该约束优化问题可以用拉格朗日乘子法求得闭式解。我们将把等式约束引入拉格朗日方程，但不等式约束可以放心地忽略——优化解任然能为$A_{ij}$产生一个正值。因此我们构建如下的拉格朗日函数：

$$
\mathcal{L}(A,\alpha)=\sum_{i=1}^{|S|}\sum_{j=1}^{|S|}\sum_{t=1}^{T}1\{z_{t-1}=s_i\wedge z_t=s_j\}logA_{ij}+\sum_{i=1}^{|S|}\alpha_i(1-\sum_{j=1}^{|S|}A_{ij})
$$

求偏导数，令它们等于零，得到:

$$
\begin{aligned}
\frac{\partial\mathcal{L}(A,\alpha)}{\partial A_{ij}} &=\frac{\partial}{\partial A_{ij}}(\sum_{t=1}^{T}1\{z_{t-1}=s_i\wedge z_t=s_j\}logA_{ij}) + \frac{\partial}{\partial A_{ij}}\alpha_i(1-\sum_{j=1}^{|S|}A_{ij}) \\
&= \frac 1{A_{ij}}\sum_{t=1}^{T}1\{z_{t-1}=s_i\wedge z_t=s_j\}-\alpha_i\equiv0\\
&\Rightarrow \\
A_{ij} &=\frac 1{\alpha_i}\sum_{t=1}^{T}1\{z_{t-1}=s_i\wedge z_t=s_j\}
\end{aligned}
$$

带入，零偏导$\alpha$等于零：

$$
\begin{aligned}
\frac{\partial\mathcal{L}(A,\alpha)}{\partial \alpha_i} &= 1-\sum_{j=1}^{|S|}A_{ij} \\
&= 1-\sum_{j=1}^{|S|}\frac 1{\alpha_i}\sum_{t=1}^{T}1\{z_{t-1}=s_i\wedge z_t=s_j\}\equiv0 \\
&\Rightarrow \\
\alpha_i &= \sum_{j=1}^{|S|}\sum_{t=1}^{T}1\{z_{t-1}=s_i\wedge z_t=s_j\} \\
&= \sum_{t=1}^{T}1\{z_{t-1}=s_i\}
\end{aligned}
$$

把$\alpha_i$带入表达式，我们推导出$A_{ij}$的最大似然参数值$\hat{A_{ij}}$为：

$$
\hat{A_{ij}} = \frac{\sum_{t=1}^T 1\{z_{t-1}=s_i\wedge z_t=s_j\}}{\sum_{t=1}^T 1\{z_{t-1} = s_i\}}
$$

这个公式编码一个简单的解释是：从状态$i$到状态$j$转移的最大似然概率其实就是从状态$i$到状态$j$出现的次数数除以总次数。换句话说，最大似然参数对应我们从状态$i$到状态$j$的次数的分数。

#### 2. 隐马尔科夫模型

马尔可夫模型是对时间序列数据的一种强大抽象，但无法捕获非常常见的场景。如果我们不能观察状态本身，而只能观察这些状态的一些概率函数，我们怎么能对一系列状态进行推理呢？比如一个词性标注的场景，其中单词被观察到，但是词性标记没有。或者在语音识别的场景中，语音序列被观察到，但是生成它的单词没有。举个简单的例子，让我们借用Jason Eisner在2002[1]`参考资料[1]见文章最下方`年提出的设置，即“冰淇淋气候学”：

情境：在2799年，你是一位气候学家，研究全球变暖的历史。你找不到巴尔的摩(Baltimore)天气的任何记录，但你找到了我（杰森·艾斯纳(Jason Eisner)）的日记。我勤奋地记录我每天吃了多少冰淇淋。关于那个夏天的天气情况，你能推断出什么？

可以使用隐马尔可夫模型(HMM)来研究这个场景。我们不能观察状态的实际序列（每天的天气）。相反，我们只能观察每个天气状态产生的一些结果（那天吃了多少冰淇淋）。

形式上，HMM是一个马尔可夫模型，我们有一系列观察到的输出$x=\{x_1,x_2,\dots,x_T\}$，该输出来自于一组输出符号集(an output alphabet)$V=\{v_1,v_2,\dots,v_{|V|}\}$，即$x_t\in V,t=1..T$。和上一节一样，我们也假定了一系列状态的存在，这些状态来自于一个状态符号集合$S=\{s_1,s_2,\dots s_{|s|}\}，z_t\in S,t=1..T$，但是在这种情况下，状态值是不可见的。状态$i$和$j$之间的转换将再次用状态转换矩阵$A_{ij}$中的对应值表示。

我们还将生成输出观测值的概率作为隐状态的函数来建模。为此，我们做了输出无关的假设(output independence assumption)，同时定义$P(x_t=v_k|z_t=s_j)=P(x_t=v_k|x_1,\dots,x_T,z_1,\dots,z_T)=B_{jk}$。矩阵$B$编码了隐藏状态产生输出$v_k$的概率，$v_k$在相应时间产生的状态是$s_j$。

回到天气的例子，假设你有四天的冰淇淋消费记录$\vec{x}=\{x_1=v_3,x_2=v_2,x_3=v_1,x_4=v_2\}$。其中我们的观察集合仅仅有冰激凌消耗的数量，即$V=\{v_1=1冰激凌,v_2=2冰激凌,v_3=3冰激凌\}$。HMM能给我们回答什么问题呢？



