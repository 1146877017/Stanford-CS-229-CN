# CS229 课程讲义中文翻译
CS229 Lecture notes

|原作者|翻译|
|--|--|
|[Andrew Ng  吴恩达](http://www.andrewng.org/)|[CycleUser](https://www.zhihu.com/people/cycleuser/columns)|

|相关链接|
|--|
|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|
|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|
|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|
|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|


# 第一章

## 监督学习（Supervised learning）

咱们先来聊几个使用监督学习来解决问题的实例。假如咱们有一个数据集，里面的数据是俄勒冈州波特兰市的 47 套房屋的面积和价格：

|居住面积（平方英尺）|价格（千美元）|
|--|--|
|2104|400|
|1600|330|
|2400|369|
|1416|232|
|3000|540|
|...|...|
|...|...|
|...|...|

用这些数据来投个图：
![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note1f1.png)

这里要先规范一下符号和含义，这些符号以后还要用到，咱们假设 $x^{(i)}$ 表示 “输入的” 变量值（在这个例子中就是房屋面积），也可以叫做**输入特征**；然后咱们用 $y^{(i)}$ 来表示“输出值”，或者称之为**目标变量**，这个例子里面就是房屋价格。这样的一对 $(x^{(i)},y^{(i)})$就称为一组训练样本，然后咱们用来让机器来学习的数据集，就是一个长度为 m 的训练样本的列表-$\{(x^{(i)},y^{(i)}); i = 1,...,m\}$-也叫做一个**训练集**。另外一定注意，这里的上标 $“^{(i)}”$ 只是作为训练集的索引记号，和数学乘方没有任何关系，千万别误解了。另外我们还会用大写的 $X$ 来表示 **输入值的空间**，大写的 $Y$ 表示** 输出值的空间**。在本节的这个例子中，输入输出的空间都是实数域，所以 $X = Y = R$。

然后再用更加规范的方式来描述一下监督学习问题，我们的目标是，给定一个训练集，来让机器学习一个函数 $h: X → Y$，让 $h(x)$ 能是一个与对应的真实 $y$ 值比较接近的评估值。由于一些历史上的原因，这个函数 $h$ 就被叫做**假设（hypothesis）**。用一个图来表示的话，这个过程大概就是下面这样：

![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note1f2.png)

如果我们要预测的目标变量是连续的，比如在咱们这个**房屋价格-面积**的案例中，这种学习问题就被称为回归问题。如果 $y$ 只能取一小部分的离散的值（比如给定房屋面积，咱们要来确定这个房子是一个住宅还是公寓），这样的问题就叫做**分类问题**。


### 第一部分 线性回归

为了让我们的房屋案例更有意思，咱们稍微对数据集进行一下补充，增加上每一个房屋的卧室数目：

|居住面积（平方英尺）|卧室数目|价格（千美元）|
|--|--|--|
|2104|3|400|
|1600|3|330|
|2400|3|369|
|1416|2|232|
|3000|4|540|
|...|...|...|
|...|...|...|
|...|...|...|

现在，输入特征 $x$ 就是在 $R^2$ 范围取值的一个二维向量了。例如 $x_1^{(i)}$ 就是训练集中第 $i$ 个房屋的面积，而 $x_2^{(i)}$  就是训练集中第 $i$ 个房屋的卧室数目。（通常来说，设计一个学习算法的时候，选择那些输入特征都取决于你，所以如果你不在波特兰收集房屋信息数据，你也完全可以选择包含其他的特征，例如房屋是否有壁炉，卫生间的数量啊等等。关于特征筛选的内容会在后面的章节进行更详细的介绍，不过目前来说就暂时先用给定的这两个特征了。）
要进行这个监督学习，咱们必须得确定好如何在计算机里面对这个**函数/假设** $h$ 进行表示。咱们现在刚刚开始，就来个简单点的，咱们把 $y$ 假设为一个以 $x$ 为变量的线性函数：

$ h_\theta  (x) = \theta_0 + \theta_1 \times x_1 + \theta_2 \times x_2$

这里的$\theta_i$是**参数**（也可以叫做**权重**），是从 $X$ 到 $Y$ 的线性函数映射的空间参数。在不至于引起混淆的情况下，咱们可以把$h_\theta(x)$ 里面的 $\theta$  省略掉，就简写成 $h(x$)。另外为了简化公式，咱们还设 $x_0 = 1$（这个为 **截距项 intercept term**）。这样简化之后就有了：




